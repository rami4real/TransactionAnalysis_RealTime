# ğŸš€ Big Data Pipeline for Real-Time Banking Transaction Analysis

## Kafka | Hadoop (HDFS) | Hive | Airflow | Superset | Hue | Zookeeper

---

## ğŸ“¸ AperÃ§u du Projet
ğŸ” Ce projet acadÃ©mique met en place un pipeline Big Data permettant l'ingestion, le traitement, l'analyse et la visualisation de donnÃ©es de transactions bancaires en temps rÃ©el.

![Dashboard](images/final_dash.png)

### Technologies clÃ©s :
- **Kafka** â€“ Ingestion en temps rÃ©el
- **HDFS (Hadoop)** â€“ Stockage distribuÃ©
- **Hive** â€“ Analyse SQL des donnÃ©es stockÃ©es
- **Airflow** â€“ Orchestration du pipeline
- **Hue** â€“ Gestion et requÃªtage SQL sur Hive
- **Superset** â€“ Visualisation et crÃ©ation de dashboards interactifs
- **Zookeeper** â€“ Coordination des services Kafka et Hadoop

---

## ğŸ“‚ Architecture du Projet

### Flux de donnÃ©es :
1. DÃ©pÃ´t de fichiers CSV dans un dossier surveillÃ©  
2. Kafka ingÃ¨re les fichiers et transmet leurs chemins  
3. Airflow consomme les fichiers et les ingÃ¨re dans HDFS  
4. Hive charge les fichiers depuis HDFS vers des tables SQL  
5. Hue permet des requÃªtes SQL pour analyser les donnÃ©es  
6. Superset crÃ©e des visualisations dynamiques et des dashboards  

---

## ğŸ¯ Objectifs du Projet
- Automatiser lâ€™ingestion et le traitement des donnÃ©es massives
- GÃ©rer efficacement le stockage des fichiers via Hadoop HDFS
- Analyser les transactions grÃ¢ce Ã  Hive et SQL
- Visualiser les rÃ©sultats via Superset pour dÃ©tecter des tendances et fraudes bancaires

---

## âš™ï¸ Technologies UtilisÃ©es

| Technologie  | RÃ´le                                             |
|-------------|--------------------------------------------------|
| **Kafka**   | Streaming et ingestion en temps rÃ©el              |
| **HDFS**    | Stockage distribuÃ© de fichiers CSV                |
| **Hive**    | Analyse et requÃªtes SQL sur les donnÃ©es           |
| **Airflow** | Orchestration des tÃ¢ches et automatisation        |
| **Hue**     | Interface web pour requÃªtes SQL sur Hive          |
| **Superset**| Visualisation des donnÃ©es et dashboards           |
| **Zookeeper**| Coordination de Kafka et Hadoop                  |

---

## ğŸ› ï¸ Installation et DÃ©ploiement

Le projet est dÃ©ployÃ© sur un cluster (1 master et 2 slaves). Pour rÃ©cupÃ©rer les configurations nÃ©cessaires :
```bash
git clone https://github.com/Dhia-69/TransactionAnalysis_RealTime 
cd TransactionAnalysis_RealTime
```

### Lancement des Services
Consultez le fichier **"Lancement des services.txt"** pour exÃ©cuter les services nÃ©cessaires.

---

## ğŸ“Š Exemples de Dashboards et Interaction avec les Technologies
**Dashboard Apache Superset**
![DashboardFinal](images/final_dash.png)
![Exemple](images/dah.png)
**FonctionalitÃ©s et verification avec Apache Hue**
![RequeteSql](images/hue.jpg)
![Jobs](images/hue2.jpg)
![Vis](images/hue3.jpg)
**Interface Apache Airflow**
![Airflow](images/airflow.png)

## ğŸš€ ExÃ©cution du Pipeline
1. **Kafka Producer** : DÃ©pose les fichiers CSV dans le dossier surveillÃ© (`~/kafka_ingestion`).
2. **Kafka Consumer** : Consomme les messages Kafka et dÃ©place les fichiers vers HDFS.
3. **Hive** : Charge les fichiers HDFS et met Ã  jour la table Hive.
4. **Superset** : Visualise automatiquement les nouvelles donnÃ©es grÃ¢ce Ã  la connexion JDBC avec Hive.

---

## ğŸ¥ DÃ©mo VidÃ©o
ğŸ”— [https://drive.google.com/file/d/154iiTMNqWlyOTbEOYxhQxQMX98fD5USk/view?usp=sharing](#)  
